{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwKdQ752ZSn5"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import io\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GEMINI_API_KEY = \"AIzaSyAjYp27jimJr1Ivs9-SKcYaTJe4XUKQUbE\"\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "ANSWER_KEY = {\n",
        "    1: \"False\",\n",
        "    2: \"False\",\n",
        "    3: \"False\",\n",
        "    4: \"False\",\n",
        "    5: \"False\",\n",
        "    6: \"False\",\n",
        "    7: \"True\",\n",
        "    8: \"True\",\n",
        "    9: \"True\",\n",
        "    10: \"True\"\n",
        "}"
      ],
      "metadata": {
        "id": "Dijvp-MxZdCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_answers_from_image(image: Image.Image) -> Dict[int, str]:\n",
        "    prompt = \"\"\"You are an expert OCR system specialized in reading handwritten True/False answers from academic answer sheets.\n",
        "\n",
        "TASK: Carefully analyze this answer sheet image and extract ONLY the handwritten True/False answers for questions 1 through 10.\n",
        "\n",
        "IMPORTANT INSTRUCTIONS:\n",
        "1. Focus ONLY on the handwritten text in the \"TRUE/FALSE\" column (rightmost column)\n",
        "2. Ignore any underlines, scribbles, circles, or extra markings\n",
        "3. Each answer should be either \"True\" or \"False\" (case-insensitive)\n",
        "4. Handle variations in handwriting: messy writing, slanted text, cursive, print, capitalization variations\n",
        "5. If text is crossed out and rewritten, use the final uncrossed answer\n",
        "6. Ignore any text in the \"Statement\" column or row headers\n",
        "7. Extract exactly 10 answers corresponding to questions 1-10\n",
        "\n",
        "HANDWRITING CHALLENGES TO HANDLE:\n",
        "- Unclear letters (e.g., 'a' vs 'o', 'l' vs '1')\n",
        "- Cursive writing where letters blend\n",
        "- Partial words or abbreviations (e.g., \"T\" for True, \"F\" for False)\n",
        "- Mixed case (e.g., \"TRue\", \"fAlse\", \"TRUE\", \"false\")\n",
        "- Additional marks like check marks, underlines, or strikethroughs\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "Return your response as a JSON object with question numbers as keys and extracted answers as values.\n",
        "Use this exact format:\n",
        "{\n",
        "  \"1\": \"True\",\n",
        "  \"2\": \"False\",\n",
        "  \"3\": \"True\",\n",
        "  ...\n",
        "  \"10\": \"False\"\n",
        "}\n",
        "\n",
        "Only return the JSON object, nothing else. Ensure all 10 answers are included.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content([prompt, image])\n",
        "\n",
        "        response_text = response.text.strip()\n",
        "\n",
        "        json_match = re.search(r'\\{[^}]+\\}', response_text, re.DOTALL)\n",
        "        if json_match:\n",
        "            json_str = json_match.group()\n",
        "            answers_dict = json.loads(json_str)\n",
        "\n",
        "            normalized_answers = {}\n",
        "            for key, value in answers_dict.items():\n",
        "                q_num = int(key)\n",
        "                answer_normalized = str(value).strip().lower()\n",
        "                if answer_normalized.startswith('t'):\n",
        "                    normalized_answers[q_num] = \"True\"\n",
        "                elif answer_normalized.startswith('f'):\n",
        "                    normalized_answers[q_num] = \"False\"\n",
        "                else:\n",
        "                    normalized_answers[q_num] = \"Unknown\"\n",
        "\n",
        "            return normalized_answers\n",
        "        else:\n",
        "            raise ValueError(\"Could not extract JSON from response\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extraction: {str(e)}\")\n",
        "        print(f\"Raw response: {response_text if 'response_text' in locals() else 'No response'}\")\n",
        "        return {i: \"Error\" for i in range(1, 11)}"
      ],
      "metadata": {
        "id": "Eno9LcGHZiMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_answers(extracted_answers: Dict[int, str]) -> Tuple[List[Dict], int, int]:\n",
        "    detailed_results = []\n",
        "    score = 0\n",
        "    total = 10\n",
        "\n",
        "    for i in range(1, 11):\n",
        "        student_answer = extracted_answers.get(i, \"Not Found\")\n",
        "        correct_answer = ANSWER_KEY[i]\n",
        "\n",
        "        is_correct = (student_answer == correct_answer)\n",
        "\n",
        "        if is_correct:\n",
        "            score += 1\n",
        "\n",
        "        result = {\n",
        "            \"question_num\": i,\n",
        "            \"student_answer\": student_answer,\n",
        "            \"correct_answer\": correct_answer,\n",
        "            \"result\": \"✓ Correct\" if is_correct else \"✗ Incorrect\"\n",
        "        }\n",
        "        detailed_results.append(result)\n",
        "\n",
        "    return detailed_results, score, total"
      ],
      "metadata": {
        "id": "T6RTkl9xZkqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_uploaded_image(image: Image.Image) -> str:\n",
        "    if image is None:\n",
        "        return \"Please upload an image first!\"\n",
        "\n",
        "    try:\n",
        "        print(\"Extracting handwritten answers from image...\")\n",
        "        extracted_answers = extract_answers_from_image(image)\n",
        "\n",
        "        print(\"Evaluating answers against correct key...\")\n",
        "        detailed_results, score, total = evaluate_answers(extracted_answers)\n",
        "\n",
        "        output = []\n",
        "        output.append(\"=\" * 80)\n",
        "        output.append(\"ANSWER SHEET EVALUATION RESULTS\")\n",
        "        output.append(\"=\" * 80)\n",
        "        output.append(\"\")\n",
        "\n",
        "        output.append(f\"{'Q#':<5} {'Student Answer':<20} {'Correct Answer':<20} {'Result':<15}\")\n",
        "        output.append(\"-\" * 80)\n",
        "\n",
        "        for result in detailed_results:\n",
        "            output.append(\n",
        "                f\"{result['question_num']:<5} \"\n",
        "                f\"{result['student_answer']:<20} \"\n",
        "                f\"{result['correct_answer']:<20} \"\n",
        "                f\"{result['result']:<15}\"\n",
        "            )\n",
        "\n",
        "        output.append(\"-\" * 80)\n",
        "        output.append(\"\")\n",
        "        output.append(f\"TOTAL SCORE: {score}/{total}\")\n",
        "        output.append(f\"Percentage: {(score/total)*100:.1f}%\")\n",
        "        output.append(\"\")\n",
        "\n",
        "        percentage = (score/total) * 100\n",
        "        if percentage >= 90:\n",
        "            grade = \"A+ (Excellent!)\"\n",
        "        elif percentage >= 80:\n",
        "            grade = \"A (Very Good)\"\n",
        "        elif percentage >= 70:\n",
        "            grade = \"B (Good)\"\n",
        "        elif percentage >= 60:\n",
        "            grade = \"C (Satisfactory)\"\n",
        "        else:\n",
        "            grade = \"D (Needs Improvement)\"\n",
        "\n",
        "        output.append(f\"Grade: {grade}\")\n",
        "        output.append(\"=\" * 80)\n",
        "\n",
        "        return \"\\n\".join(output)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing image: {str(e)}\\n\\nPlease ensure:\\n1. Image is clear and readable\\n2. Answer sheet follows the expected format\\n3. API key is correctly configured\""
      ],
      "metadata": {
        "id": "5Qsa0AfwZmhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Answer Sheet Evaluation System\")\n",
        "    gr.Markdown(\n",
        "        \"Upload a handwritten True/False answer sheet and get instant automated grading using Google Gemini AI\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            image_input = gr.Image(\n",
        "                type=\"pil\",\n",
        "                label=\"Upload Answer Sheet Image\",\n",
        "                height=400\n",
        "            )\n",
        "\n",
        "            evaluate_btn = gr.Button(\n",
        "                \"Evaluate Answer Sheet\",\n",
        "                variant=\"primary\",\n",
        "                size=\"lg\"\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### Instructions:\n",
        "            1. Upload a clear image of the answer sheet\n",
        "            2. Ensure handwriting is visible\n",
        "            3. Click 'Evaluate' to get results\n",
        "            \"\"\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            output_text = gr.Textbox(\n",
        "                label=\"Evaluation Results\",\n",
        "                lines=25,\n",
        "                max_lines=30,\n",
        "                show_copy_button=True\n",
        "            )\n",
        "\n",
        "    evaluate_btn.click(\n",
        "        fn=evaluate_uploaded_image,\n",
        "        inputs=image_input,\n",
        "        outputs=output_text\n",
        "    )"
      ],
      "metadata": {
        "id": "GK76VYy7hoj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Answer Sheet Evaluation System...\")\n",
        "    print(\"Configuring Gemini API...\")\n",
        "    print(\"Launching Gradio interface...\")\n",
        "\n",
        "    demo.launch(\n",
        "        share=True,\n",
        "        debug=True,\n",
        "        show_error=True\n",
        "    )\n",
        "\n",
        "    print(\"\\nApplication is running!\")\n",
        "    print(\"Access the interface using the URL above\")"
      ],
      "metadata": {
        "id": "4IsAgsKRZsMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IAv_9xyRhr4R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}